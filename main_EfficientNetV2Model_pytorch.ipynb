{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd /content/drive/MyDrive/dacon/lowresol/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !unzip -qn open.zip -d ./open/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEPRECATION: pytorch-lightning 1.7.7 has a non-standard dependency specifier torch>=1.9.*. pip 24.0 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of pytorch-lightning or contact the author to suggest that they release a version with a conforming dependency specifiers. Discussion can be found at https://github.com/pypa/pip/issues/12063\n"
     ]
    }
   ],
   "source": [
    "!pip install --quiet timm pytorch_lightning==1.7.7 torchmetrics==0.11.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Seo\\anaconda3\\envs\\Dron_PytorchLightning\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import gc\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import pytorch_lightning as L\n",
    "from torch import optim\n",
    "\n",
    "from torchinfo import summary\n",
    "#from glob import glob\n",
    "import glob\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from torchvision.io import read_image\n",
    "from torchvision.transforms import v2 as  transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import Swinv2Config, Swinv2Model, AutoImageProcessor, AutoModelForImageClassification\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "\n",
    "from pytorch_lightning.loggers import WandbLogger  # wandb logger를 임포트\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    NUM_DEVICES = torch.cuda.device_count()\n",
    "    NUM_WORKERS = os.cpu_count()\n",
    "    #NUM_CLASSES = 4\n",
    "    NUM_CLASSES = 8\n",
    "    EPOCHS = 16\n",
    "    # BATCH_SIZE = (\n",
    "    #     32 if torch.cuda.device_count() < 2 \n",
    "    #     else (32 * torch.cuda.device_count())\n",
    "    # )\n",
    "    BATCH_SIZE = 16\n",
    "    LR = 0.001\n",
    "    APPLY_SHUFFLE = True\n",
    "    SEED = 768\n",
    "    #HEIGHT = 224\n",
    "    #WIDTH = 224\n",
    "    HEIGHT = 224\n",
    "    WIDTH = 224\n",
    "    CHANNELS = 3\n",
    "    #IMAGE_SIZE = (224, 224, 3)\n",
    "    IMAGE_SIZE = (224, 224, 3)\n",
    "    \n",
    "    # Define paths\n",
    "    #DATASET_PATH = \"/content/drive/MyDrive/Colab Notebooks/dataset\"\n",
    "    #TRAIN_PATH = '/content/drive/MyDrive/Colab Notebooks/dataset/train/'\n",
    "    #TEST_PATH = '/content/drive/MyDrive/Colab Notebooks/dataset/test'\n",
    "    \n",
    "    # Define paths\n",
    "    # DATASET_PATH = \"C:\\\\Users\\\\Seo\\\\Desktop\\\\2024_06_Dron_Competition\\\\Data\"\n",
    "    # TRAIN_PATH = 'C:\\\\Users\\\\Seo\\\\Desktop\\\\2024_06_Dron_Competition\\\\Data\\\\train\\\\'\n",
    "    # TEST_PATH = 'C:\\\\Users\\\\Seo\\\\Desktop\\\\2024_06_Dron_Competition\\\\Data\\\\test\\\\'\n",
    "\n",
    "    # Define paths\n",
    "    DATASET_PATH = \"C:\\\\Users\\\\Seo\\\\Desktop\\\\Gits\\\\2024_06_Dron_Competition\\\\Datas_3000_3000\"\n",
    "    TRAIN_PATH = 'C:\\\\Users\\\\Seo\\\\Desktop\\\\Gits\\\\2024_06_Dron_Competition\\\\Datas_3000_3000\\\\'\n",
    "    TEST_PATH = 'C:\\\\Users\\\\Seo\\\\Desktop\\\\Gits\\\\2024_06_Dron_Competition\\\\Datas_3000_2250\\\\'\n",
    "    \n",
    "    \n",
    "# Mute warnings\n",
    "warnings.filterwarnings(\"ignore\", \"is_categorical_dtype\")\n",
    "warnings.filterwarnings(\"ignore\", \"use_inf_as_na\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = 'saved'\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "\n",
    "result_dir = 'result'\n",
    "if not os.path.exists(result_dir):\n",
    "    os.makedirs(result_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='1.1'></a>\n",
    "### Get image paths with glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%time\n",
    "#print(f\"{CFG.TRAIN_PATH}**/*.jpg\")\n",
    "train_images = glob.glob(f\"{CFG.TRAIN_PATH}**/*.jpg\")\n",
    "valid_images = glob.glob(f\"{CFG.TEST_PATH}**/*.jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='1.2'></a>\n",
    "### Create Pandas DataFrames for paths and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_labels(image_paths):\n",
    "    return [(_.split('\\\\')[-2:][0]).replace('-', '_') for _ in image_paths]\n",
    "\n",
    "\n",
    "def build_df(image_paths, labels):\n",
    "    # Create dataframe\n",
    "    df = pd.DataFrame({\n",
    "        'image_path': image_paths,\n",
    "        'label': generate_labels(labels)\n",
    "    })\n",
    "    \n",
    "    # Return df\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_path</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C:\\Users\\Seo\\Desktop\\Gits\\2024_06_Dron_Competi...</td>\n",
       "      <td>Bonobono</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C:\\Users\\Seo\\Desktop\\Gits\\2024_06_Dron_Competi...</td>\n",
       "      <td>Bonobono</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C:\\Users\\Seo\\Desktop\\Gits\\2024_06_Dron_Competi...</td>\n",
       "      <td>Bonobono</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C:\\Users\\Seo\\Desktop\\Gits\\2024_06_Dron_Competi...</td>\n",
       "      <td>Bonobono</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C:\\Users\\Seo\\Desktop\\Gits\\2024_06_Dron_Competi...</td>\n",
       "      <td>Bonobono</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          image_path     label\n",
       "0  C:\\Users\\Seo\\Desktop\\Gits\\2024_06_Dron_Competi...  Bonobono\n",
       "1  C:\\Users\\Seo\\Desktop\\Gits\\2024_06_Dron_Competi...  Bonobono\n",
       "2  C:\\Users\\Seo\\Desktop\\Gits\\2024_06_Dron_Competi...  Bonobono\n",
       "3  C:\\Users\\Seo\\Desktop\\Gits\\2024_06_Dron_Competi...  Bonobono\n",
       "4  C:\\Users\\Seo\\Desktop\\Gits\\2024_06_Dron_Competi...  Bonobono"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_no_kfold_df = build_df(train_images, train_images)\n",
    "valid_no_kfold_df = build_df(valid_images, valid_images)\n",
    "\n",
    "# View first 5 samples in the dataset\n",
    "train_no_kfold_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_path</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C:\\Users\\Seo\\Desktop\\Gits\\2024_06_Dron_Competi...</td>\n",
       "      <td>Bonobono</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C:\\Users\\Seo\\Desktop\\Gits\\2024_06_Dron_Competi...</td>\n",
       "      <td>Bonobono</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C:\\Users\\Seo\\Desktop\\Gits\\2024_06_Dron_Competi...</td>\n",
       "      <td>Bonobono</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C:\\Users\\Seo\\Desktop\\Gits\\2024_06_Dron_Competi...</td>\n",
       "      <td>Bonobono</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C:\\Users\\Seo\\Desktop\\Gits\\2024_06_Dron_Competi...</td>\n",
       "      <td>Bonobono</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          image_path     label\n",
       "0  C:\\Users\\Seo\\Desktop\\Gits\\2024_06_Dron_Competi...  Bonobono\n",
       "1  C:\\Users\\Seo\\Desktop\\Gits\\2024_06_Dron_Competi...  Bonobono\n",
       "2  C:\\Users\\Seo\\Desktop\\Gits\\2024_06_Dron_Competi...  Bonobono\n",
       "3  C:\\Users\\Seo\\Desktop\\Gits\\2024_06_Dron_Competi...  Bonobono\n",
       "4  C:\\Users\\Seo\\Desktop\\Gits\\2024_06_Dron_Competi...  Bonobono"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_no_kfold_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2424, 2), (2424, 2))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_no_kfold_df.shape, valid_no_kfold_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='1.3'></a>\n",
    "### Load & View Random Sample Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _load(image_path, as_tensor=True):\n",
    "    # Read and decode an image file to a uint8 tensor\n",
    "    image = Image.open(image_path)\n",
    "    \n",
    "    if as_tensor:\n",
    "        converter = transforms.Compose([\n",
    "            transforms.ToTensor(), \n",
    "            #transforms.Grayscale()\n",
    "        ])\n",
    "        return converter(image)\n",
    "    else:\n",
    "        return image\n",
    "\n",
    "\n",
    "def view_sample(image, label, color_map='rgb', fig_size=(8, 10)):\n",
    "    plt.figure(figsize=fig_size)\n",
    "    \n",
    "    if color_map=='rgb':\n",
    "        plt.imshow(image)\n",
    "    else:\n",
    "        plt.imshow(image, cmap=color_map)\n",
    "    \n",
    "    plt.title(f'Label: {label}', fontsize=16)\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='1.5'></a>\n",
    "### Create Train and Validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, df, path_col,  mode='train'):\n",
    "        self.df = df\n",
    "        self.path_col = path_col\n",
    "        self.mode = mode\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.mode == 'train':\n",
    "            row = self.df.iloc[idx]\n",
    "            image = read_image(row[self.path_col])/256.\n",
    "            label = row['class']\n",
    "            data = {\n",
    "                'image':image,\n",
    "                'label':label\n",
    "            }\n",
    "            return data\n",
    "        elif self.mode == 'val':\n",
    "            row = self.df.iloc[idx]\n",
    "            image = read_image(row[self.path_col])/256.\n",
    "            label = row['class']\n",
    "            data = {\n",
    "                'image':image,\n",
    "                'label':label\n",
    "            }\n",
    "            return data\n",
    "        elif self.mode == 'inference':\n",
    "            row = self.df.iloc[idx]\n",
    "            image = read_image(row[self.path_col])/256.\n",
    "            data = {\n",
    "                'image':image,\n",
    "            }\n",
    "            return data\n",
    "\n",
    "    def train_transform(self, image):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomCollateFn:\n",
    "    def __init__(self, transform, mode):\n",
    "        self.mode = mode\n",
    "        self.transform = transform\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        if self.mode=='train':\n",
    "            images = torch.stack([self.transform(data['image']) for data in batch])\n",
    "            labels = torch.LongTensor([data['label'] for data in batch])\n",
    "            return images, labels\n",
    "        elif self.mode=='val':\n",
    "            images = torch.stack([self.transform(data['image']) for data in batch])\n",
    "            labels = torch.LongTensor([data['label'] for data in batch])\n",
    "            return images, labels\n",
    "        elif self.mode=='inference':\n",
    "            images = torch.stack([self.transform(data['image']) for data in batch])\n",
    "            return images, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "N_SPLIT = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L.seed_everything(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_or_load_class_mapping(df, pickle_file_path):\n",
    "    # 미리 정의된 클래스 매핑\n",
    "    class_mapping = {\n",
    "        'Bonobono': 0,\n",
    "        'Eagle': 1,\n",
    "        'Gmarket': 2,\n",
    "        'Hp': 3,\n",
    "        'Intel': 4,\n",
    "        'underwood_statue': 5,\n",
    "        'Wonju': 6,\n",
    "        'Yonsei': 7\n",
    "    }\n",
    "    \n",
    "    # pickle 파일이 존재하지 않으면 생성\n",
    "    if not os.path.exists(pickle_file_path):\n",
    "        print(\"Pickle 파일을 생성합니다.\")\n",
    "        df['class'] = df['label'].map(class_mapping)\n",
    "        \n",
    "        with open(pickle_file_path, 'wb') as f:\n",
    "            pickle.dump(class_mapping, f)\n",
    "        \n",
    "        return class_mapping\n",
    "    else:\n",
    "        # pickle 파일이 존재하면 로드\n",
    "        print(\"Pickle 파일을 로드합니다.\")\n",
    "        with open(pickle_file_path, 'rb') as f:\n",
    "            class_mapping = pickle.load(f)\n",
    "        \n",
    "        return class_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pickle 파일을 로드합니다.\n"
     ]
    }
   ],
   "source": [
    "# 함수를 호출하여 class_mapping 생성 또는 로드\n",
    "pickle_file_path = 'C:\\\\Users\\\\Seo\\\\Desktop\\\\Gits\\\\2024_06_Dron_Competition\\\\class8.pickle'\n",
    "class_mapping = create_or_load_class_mapping(train_no_kfold_df, pickle_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 클래스 매핑을 이용하여 라벨을 클래스 번호로 변환 (예시로 train_no_kfold_df 사용)\n",
    "train_no_kfold_df['class'] = train_no_kfold_df['label'].apply(lambda x: class_mapping.get(x))\n",
    "valid_no_kfold_df['class'] = valid_no_kfold_df['label'].apply(lambda x: class_mapping.get(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "le_no_kfold = LabelEncoder()\n",
    "train_no_kfold_df['class'] = le_no_kfold.fit_transform(train_no_kfold_df['label'])\n",
    "valid_no_kfold_df['class'] = le_no_kfold.fit_transform(valid_no_kfold_df['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "# 예시 데이터프레임 생성\n",
    "# valid_no_kfold_df = pd.DataFrame({\n",
    "#     'label': [...],\n",
    "#     'class': [...]\n",
    "# })\n",
    "\n",
    "# 샘플링할 데이터 비율 (예: 전체 데이터의 20%)\n",
    "sample_ratio = 0.4\n",
    "\n",
    "# StratifiedShuffleSplit 객체 생성\n",
    "splitter = StratifiedShuffleSplit(n_splits=1, test_size=sample_ratio, random_state=42)\n",
    "\n",
    "# 샘플링\n",
    "for train_index, sample_index in splitter.split(valid_no_kfold_df, valid_no_kfold_df['class']):\n",
    "    sampled_df = valid_no_kfold_df.iloc[sample_index]\n",
    "\n",
    "valid_no_kfold_df = sampled_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if not len(train_df) == len(os.listdir('./open/train')):\n",
    "#    raise ValueError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import v2 as  transforms\n",
    "from torchvision.transforms import RandomAffine, RandomHorizontalFlip, RandomVerticalFlip, ColorJitter\n",
    "\n",
    "def add_random_noise(image):\n",
    "    noise = torch.randn_like(image) * 0.1\n",
    "    return image + noise\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize(size=(CFG.WIDTH,CFG.WIDTH), interpolation=transforms.InterpolationMode.BICUBIC),\n",
    "    RandomAffine(degrees=30, translate=(0.2, 0.2), scale=(0.8, 1.2)),\n",
    "    transforms.RandomErasing(p=0.5, scale=(0.02, 0.33), ratio=(0.3, 3.3), value='random'), \n",
    "    RandomHorizontalFlip(p=0.5),\n",
    "    ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "    transforms.Lambda(add_random_noise),\n",
    "    # transforms.RandomGrayscale(p=0.1),\n",
    "    transforms.Normalize(mean=(0.485,0.456,0.406), std=(0.229,0.224,0.225)),\n",
    "])\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize(size=(CFG.WIDTH,CFG.WIDTH), interpolation=transforms.InterpolationMode.BICUBIC),\n",
    "    RandomAffine(degrees=30, translate=(0.2, 0.2), scale=(0.8, 1.2)),\n",
    "    RandomHorizontalFlip(p=0.5),\n",
    "    ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "    transforms.Lambda(add_random_noise),\n",
    "    transforms.Normalize(mean=(0.485,0.456,0.406), std=(0.229,0.224,0.225)),\n",
    "])\n",
    "\n",
    "train_collate_fn = CustomCollateFn(train_transform, 'train')\n",
    "val_collate_fn = CustomCollateFn(val_transform, 'val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EfficientNetV2Model(nn.Module):\n",
    "    def __init__(self, backbone_model, name='efficientnet-v2-large', \n",
    "                 num_classes=CFG.NUM_CLASSES, device=CFG.DEVICE):\n",
    "        super(EfficientNetV2Model, self).__init__()\n",
    "        \n",
    "        self.backbone_model = backbone_model\n",
    "        self.device = device\n",
    "        self.num_classes = num_classes\n",
    "        self.name = name\n",
    "        \n",
    "        classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Dropout(p=0.2, inplace=True), \n",
    "            nn.Linear(in_features=1280, out_features=256, bias=True),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(p=0.2, inplace=True),\n",
    "            nn.Linear(in_features=256, out_features=num_classes, bias=False)\n",
    "        ).to(device)\n",
    "        \n",
    "        self._set_classifier(classifier)\n",
    "        \n",
    "    def _set_classifier(self, classifier:nn.Module) -> None:\n",
    "        self.backbone_model.classifier = classifier\n",
    "    \n",
    "    def forward(self, image):\n",
    "        return self.backbone_model(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_effiecientnetv2_model(\n",
    "    device: torch.device=CFG.NUM_CLASSES) -> nn.Module:\n",
    "    # Set the manual seeds\n",
    "    torch.manual_seed(CFG.SEED)\n",
    "    torch.cuda.manual_seed(CFG.SEED)\n",
    "\n",
    "    # Get model weights\n",
    "    model_weights = (\n",
    "        torchvision\n",
    "        .models\n",
    "        .EfficientNet_V2_L_Weights\n",
    "        .DEFAULT\n",
    "    )\n",
    "    \n",
    "    # Get model and push to device\n",
    "    model = (\n",
    "        torchvision.models.efficientnet_v2_l(\n",
    "            weights=model_weights\n",
    "        )\n",
    "    ).to(device) \n",
    "    \n",
    "    # Freeze Model Parameters\n",
    "    for param in model.features.parameters():\n",
    "        param.requires_grad = False\n",
    "        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get EfficientNet v2 model\n",
    "backbone_model = get_effiecientnetv2_model(CFG.DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "efficientnetv2_params = {\n",
    "    'backbone_model'    : backbone_model,\n",
    "    'name'              : 'efficientnet-v2-large',\n",
    "    'device'            : CFG.DEVICE\n",
    "}\n",
    "\n",
    "# Generate Model\n",
    "efficientnet_model = EfficientNetV2Model(**efficientnetv2_params)\n",
    "\n",
    "# If using GPU T4 x2 setup, use this:\n",
    "if CFG.NUM_DEVICES > 1:\n",
    "    efficientnet_model = nn.DataParallel(efficientnet_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "=================================================================================================================================================\n",
       "Layer (type (var_name))                                           Input Shape          Output Shape         Param #              Trainable\n",
       "=================================================================================================================================================\n",
       "EfficientNetV2Model (EfficientNetV2Model)                         [16, 3, 224, 224]    [16, 8]              --                   Partial\n",
       "├─EfficientNet (backbone_model)                                   [16, 3, 224, 224]    [16, 8]              --                   Partial\n",
       "│    └─Sequential (features)                                      [16, 3, 224, 224]    [16, 1280, 7, 7]     --                   False\n",
       "│    │    └─Conv2dNormActivation (0)                              [16, 3, 224, 224]    [16, 32, 112, 112]   (928)                False\n",
       "│    │    └─Sequential (1)                                        [16, 32, 112, 112]   [16, 32, 112, 112]   (37,120)             False\n",
       "│    │    └─Sequential (2)                                        [16, 32, 112, 112]   [16, 64, 56, 56]     (1,032,320)          False\n",
       "│    │    └─Sequential (3)                                        [16, 64, 56, 56]     [16, 96, 28, 28]     (2,390,336)          False\n",
       "│    │    └─Sequential (4)                                        [16, 96, 28, 28]     [16, 192, 14, 14]    (3,553,224)          False\n",
       "│    │    └─Sequential (5)                                        [16, 192, 14, 14]    [16, 224, 14, 14]    (14,501,728)         False\n",
       "│    │    └─Sequential (6)                                        [16, 224, 14, 14]    [16, 384, 7, 7]      (54,866,360)         False\n",
       "│    │    └─Sequential (7)                                        [16, 384, 7, 7]      [16, 640, 7, 7]      (40,030,496)         False\n",
       "│    │    └─Conv2dNormActivation (8)                              [16, 640, 7, 7]      [16, 1280, 7, 7]     (821,760)            False\n",
       "│    └─AdaptiveAvgPool2d (avgpool)                                [16, 1280, 7, 7]     [16, 1280, 1, 1]     --                   --\n",
       "│    └─Sequential (classifier)                                    [16, 1280]           [16, 8]              --                   True\n",
       "│    │    └─Flatten (0)                                           [16, 1280]           [16, 1280]           --                   --\n",
       "│    │    └─Dropout (1)                                           [16, 1280]           [16, 1280]           --                   --\n",
       "│    │    └─Linear (2)                                            [16, 1280]           [16, 256]            327,936              True\n",
       "│    │    └─GELU (3)                                              [16, 256]            [16, 256]            --                   --\n",
       "│    │    └─Dropout (4)                                           [16, 256]            [16, 256]            --                   --\n",
       "│    │    └─Linear (5)                                            [16, 256]            [16, 8]              2,048                True\n",
       "=================================================================================================================================================\n",
       "Total params: 117,564,256\n",
       "Trainable params: 329,984\n",
       "Non-trainable params: 117,234,272\n",
       "Total mult-adds (G): 195.69\n",
       "=================================================================================================================================================\n",
       "Input size (MB): 9.63\n",
       "Forward/backward pass size (MB): 8816.61\n",
       "Params size (MB): 470.26\n",
       "Estimated Total Size (MB): 9296.50\n",
       "================================================================================================================================================="
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View model summary\n",
    "summary(\n",
    "    model=efficientnet_model, \n",
    "    input_size=(CFG.BATCH_SIZE, CFG.CHANNELS, CFG.WIDTH, CFG.HEIGHT),\n",
    "    col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
    "    col_width=20,\n",
    "    row_settings=[\"var_names\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pytorch_CustomModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Pytorch_CustomModel, self).__init__()\n",
    "        self.model = efficientnet_model\n",
    "        self.clf = nn.Sequential(\n",
    "            nn.SiLU(),\n",
    "            nn.LazyLinear(CFG.NUM_CLASSES),\n",
    "        )\n",
    "\n",
    "#     @torch.compile\n",
    "    def forward(self, x, label=None):\n",
    "        # original\n",
    "        x = self.model(x)\n",
    "        # x = self.clf(x)\n",
    "        loss = None\n",
    "        if label is not None:\n",
    "            loss = nn.CrossEntropyLoss()(x, label)\n",
    "        probs = nn.LogSoftmax(dim=-1)(x)\n",
    "        return probs, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, epoch, total_epoch, train_dataloader, criterion, optimizer):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    progress_bar = tqdm(enumerate(train_dataloader), total=len(train_dataloader), desc=f\"Epoch {epoch}/{total_epoch}\")\n",
    "    for i, (data, target) in progress_bar:\n",
    "        data, target = data.to(CFG.DEVICE), target.to(CFG.DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        output, loss_ = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "         # 손실 및 정확도 계산\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "        total += target.size(0)\n",
    "        correct += (predicted == target).sum().item()\n",
    "        # tqdm의 설명 부분을 업데이트\n",
    "        progress_bar.set_postfix(loss=running_loss/(i+1), accuracy=100.*correct/total)\n",
    "    # 에포크 완료 후 출력\n",
    "    print(f\"Epoch {epoch}, Loss: {running_loss/len(train_dataloader):.4f}, Accuracy: {correct/total:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 검증 함수\n",
    "def valid(model, val_loader, criterion):\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    correct = 0\n",
    "    progress_bar = tqdm(val_loader, total=len(val_loader))\n",
    "    with torch.no_grad():\n",
    "        for data, target in progress_bar:\n",
    "            data, target = data.to(CFG.DEVICE), target.to(CFG.DEVICE)\n",
    "            output, loss_ = model(data)\n",
    "            val_loss += criterion(output, target).item() # sum up batch loss\n",
    "            pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "            correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
    "    \n",
    "    val_loss /= len(val_loader.dataset)\n",
    "    print('Valid set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        val_loss, correct, len(val_loader.dataset),\n",
    "        100. * correct / len(val_loader.dataset)))\n",
    "    #wandb.log({\"Valid Accuracy\": 100. * correct / len(val_loader.dataset), \"Valid Loss\": val_loss})\n",
    "\n",
    "    return val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Seo\\anaconda3\\envs\\Dron_PytorchLightning\\lib\\site-packages\\torch\\nn\\modules\\lazy.py:181: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
      "  warnings.warn('Lazy modules are a new feature under heavy development '\n",
      "Epoch 0/16: 100%|██████████| 152/152 [03:58<00:00,  1.57s/it, accuracy=25.7, loss=2]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 1.9991, Accuracy: 0.2566\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 61/61 [01:10<00:00,  1.16s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid set: Average loss: 0.1216, Accuracy: 573/970 (59%)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/16: 100%|██████████| 152/152 [03:59<00:00,  1.57s/it, accuracy=31, loss=nan]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: nan, Accuracy: 0.3098\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 61/61 [01:12<00:00,  1.18s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid set: Average loss: nan, Accuracy: 126/970 (13%)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/16:  47%|████▋     | 72/152 [01:55<02:08,  1.61s/it, accuracy=11.6, loss=nan]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[29], line 24\u001b[0m\n\u001b[0;32m     20\u001b[0m best_val_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minf\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(CFG\u001b[38;5;241m.\u001b[39mEPOCHS):\n\u001b[1;32m---> 24\u001b[0m     \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mCFG\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEPOCHS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     26\u001b[0m     val_loss \u001b[38;5;241m=\u001b[39m valid(model, val_dataloader, criterion)\n\u001b[0;32m     27\u001b[0m     checkpoint_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./checkpoints\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "Cell \u001b[1;32mIn[27], line 7\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, epoch, total_epoch, train_dataloader, criterion, optimizer)\u001b[0m\n\u001b[0;32m      5\u001b[0m total \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m      6\u001b[0m progress_bar \u001b[38;5;241m=\u001b[39m tqdm(\u001b[38;5;28menumerate\u001b[39m(train_dataloader), total\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(train_dataloader), desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_epoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, (data, target) \u001b[38;5;129;01min\u001b[39;00m progress_bar:\n\u001b[0;32m      8\u001b[0m     data, target \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mto(CFG\u001b[38;5;241m.\u001b[39mDEVICE), target\u001b[38;5;241m.\u001b[39mto(CFG\u001b[38;5;241m.\u001b[39mDEVICE)\n\u001b[0;32m      9\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[1;32mc:\\Users\\Seo\\anaconda3\\envs\\Dron_PytorchLightning\\lib\\site-packages\\tqdm\\std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[0;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1181\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[0;32m   1182\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[0;32m   1183\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[0;32m   1184\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Seo\\anaconda3\\envs\\Dron_PytorchLightning\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\Seo\\anaconda3\\envs\\Dron_PytorchLightning\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\Seo\\anaconda3\\envs\\Dron_PytorchLightning\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\Seo\\anaconda3\\envs\\Dron_PytorchLightning\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[1;32mIn[11], line 14\u001b[0m, in \u001b[0;36mCustomDataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     12\u001b[0m row \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdf\u001b[38;5;241m.\u001b[39miloc[idx]\n\u001b[0;32m     13\u001b[0m image \u001b[38;5;241m=\u001b[39m read_image(row[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpath_col])\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m256.\u001b[39m\n\u001b[1;32m---> 14\u001b[0m label \u001b[38;5;241m=\u001b[39m \u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mclass\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m     15\u001b[0m data \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m'\u001b[39m:image,\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m:label\n\u001b[0;32m     18\u001b[0m }\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[1;32mc:\\Users\\Seo\\anaconda3\\envs\\Dron_PytorchLightning\\lib\\site-packages\\pandas\\core\\series.py:1016\u001b[0m, in \u001b[0;36mSeries.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   1013\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_constructor(mgr, fastpath\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m   1014\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out\u001b[38;5;241m.\u001b[39m__finalize__(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m-> 1016\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, key):\n\u001b[0;32m   1017\u001b[0m     check_dict_or_set_indexers(key)\n\u001b[0;32m   1018\u001b[0m     key \u001b[38;5;241m=\u001b[39m com\u001b[38;5;241m.\u001b[39mapply_if_callable(key, \u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_fold_df = train_no_kfold_df\n",
    "val_fold_df = valid_no_kfold_df\n",
    "\n",
    "train_dataset = CustomDataset(train_fold_df, 'image_path', mode='train')\n",
    "val_dataset = CustomDataset(val_fold_df, 'image_path', mode='val')\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, collate_fn=train_collate_fn, batch_size=CFG.BATCH_SIZE, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, collate_fn=val_collate_fn, batch_size=CFG.BATCH_SIZE)\n",
    "\n",
    "# 모델, 손실함수, 최적화함수 설정\n",
    "model = Pytorch_CustomModel().to(CFG.DEVICE)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(params=model.parameters(), lr=1e-5, weight_decay=5e-4, eps=5e-9)\n",
    "\n",
    "# wandb에 모델, 최적화 함수 로그\n",
    "#wandb.watch(model, log=\"all\")\n",
    "#wandb.config.update({\"Optimizer\": \"ADAM\", \"Learning Rate\": 0.01, \"Momentum\": 0.5})\n",
    "\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "for epoch in range(CFG.EPOCHS):\n",
    "    \n",
    "    train(model, epoch, CFG.EPOCHS, train_dataloader, criterion, optimizer)\n",
    "\n",
    "    val_loss = valid(model, val_dataloader, criterion)\n",
    "    checkpoint_dir = './checkpoints'\n",
    "    \n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "    # 모델 가중치 저장\n",
    "    checkpoint_path = os.path.join(checkpoint_dir, f'EfficientNetV2_epoch{epoch}_valid_loss{val_loss}.pth')\n",
    "    torch.save(model.state_dict(), checkpoint_path)\n",
    "    # Validation 성능이 향상될 때마다 가장 좋은 모델 가중치 저장\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_checkpoint_path = os.path.join(checkpoint_dir, f'EfficientNetV2_best_model_valid_loss{val_loss}.pth')\n",
    "        torch.save(model.state_dict(), best_checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Seo\\anaconda3\\envs\\Dron_PytorchLightning\\lib\\site-packages\\torch\\nn\\modules\\lazy.py:181: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
      "  warnings.warn('Lazy modules are a new feature under heavy development '\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_fold_df = train_no_kfold_df\n",
    "val_fold_df = valid_no_kfold_df\n",
    "\n",
    "train_dataset = CustomDataset(train_fold_df, 'image_path', mode='train')\n",
    "val_dataset = CustomDataset(val_fold_df, 'image_path', mode='val')\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, collate_fn=train_collate_fn, batch_size=CFG.BATCH_SIZE)\n",
    "val_dataloader = DataLoader(val_dataset, collate_fn=val_collate_fn, batch_size=CFG.BATCH_SIZE)\n",
    "\n",
    "model = Swinv2Model.from_pretrained(\"microsoft/swinv2-large-patch4-window12to16-192to256-22kto1k-ft\")\n",
    "#model = efficientnet_model\n",
    "\n",
    "lit_model = LitCustomModel(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "=====================================================================================================================================================================\n",
       "Layer (type (var_name))                                                               Input Shape          Output Shape         Param #              Trainable\n",
       "=====================================================================================================================================================================\n",
       "Swinv2Model (Swinv2Model)                                                             [32, 3, 224, 224]    [32, 1536]           --                   True\n",
       "├─Swinv2Embeddings (embeddings)                                                       [32, 3, 224, 224]    [32, 3136, 192]      --                   True\n",
       "│    └─Swinv2PatchEmbeddings (patch_embeddings)                                       [32, 3, 224, 224]    [32, 3136, 192]      --                   True\n",
       "│    │    └─Conv2d (projection)                                                       [32, 3, 224, 224]    [32, 192, 56, 56]    9,408                True\n",
       "│    └─LayerNorm (norm)                                                               [32, 3136, 192]      [32, 3136, 192]      384                  True\n",
       "│    └─Dropout (dropout)                                                              [32, 3136, 192]      [32, 3136, 192]      --                   --\n",
       "├─Swinv2Encoder (encoder)                                                             [32, 3136, 192]      [32, 49, 1536]       --                   True\n",
       "│    └─ModuleList (layers)                                                            --                   --                   --                   True\n",
       "│    │    └─Swinv2Stage (0)                                                           [32, 3136, 192]      [32, 784, 384]       1,194,252            True\n",
       "│    │    └─Swinv2Stage (1)                                                           [32, 784, 384]       [32, 196, 768]       4,744,728            True\n",
       "│    │    └─Swinv2Stage (2)                                                           [32, 196, 768]       [32, 49, 1536]       132,538,800          True\n",
       "│    │    └─Swinv2Stage (3)                                                           [32, 49, 1536]       [32, 49, 1536]       56,712,288           True\n",
       "├─LayerNorm (layernorm)                                                               [32, 49, 1536]       [32, 49, 1536]       3,072                True\n",
       "├─AdaptiveAvgPool1d (pooler)                                                          [32, 1536, 49]       [32, 1536, 1]        --                   --\n",
       "=====================================================================================================================================================================\n",
       "Total params: 195,202,932\n",
       "Trainable params: 195,202,932\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (G): 7.44\n",
       "=====================================================================================================================================================================\n",
       "Input size (MB): 19.27\n",
       "Forward/backward pass size (MB): 15292.76\n",
       "Params size (MB): 780.81\n",
       "Estimated Total Size (MB): 16092.84\n",
       "====================================================================================================================================================================="
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View model summary\n",
    "summary(\n",
    "    model=lit_model, \n",
    "    input_size=(CFG.BATCH_SIZE, CFG.CHANNELS, CFG.WIDTH, CFG.HEIGHT),\n",
    "    col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
    "    col_width=20,\n",
    "    row_settings=[\"var_names\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 체크포인트 파일 경로\n",
    "ckpt_path = 'C:\\\\Users\\\\Seo\\\\Desktop\\\\2024_06_Dron_Competition\\\\checkpoints\\\\swinv2-large-resize-epoch=06-train_loss=0.0012-val_score=0.9991.ckpt'\n",
    "\n",
    "# 체크포인트 로드\n",
    "checkpoint = torch.load(ckpt_path)\n",
    "\n",
    "# 모델 상태 로드\n",
    "lit_model.load_state_dict(checkpoint['state_dict'])\n",
    "\n",
    "# .pth 파일 경로\n",
    "pth_path = 'C:\\\\Users\\\\Seo\\\\Desktop\\\\2024_06_Dron_Competition\\\\swinv2-large-resize-epoch=06-train_loss=0.0012-val_score=0.9991.pth'\n",
    "\n",
    "# .pth 파일로 저장\n",
    "torch.save(lit_model.state_dict(), pth_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "CustomModel.forward() takes from 2 to 3 positional arguments but 5 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[47], line 19\u001b[0m\n\u001b[0;32m     15\u001b[0m onnx_model_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC:\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mUsers\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mSeo\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mDesktop\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124m2024_06_Dron_Competition\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mswinv2-large-resize-epoch=06-train_loss=0.0012-val_score=0.9991.onnx\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     17\u001b[0m input_shape \u001b[38;5;241m=\u001b[39m (CFG\u001b[38;5;241m.\u001b[39mBATCH_SIZE, CFG\u001b[38;5;241m.\u001b[39mCHANNELS, CFG\u001b[38;5;241m.\u001b[39mWIDTH, CFG\u001b[38;5;241m.\u001b[39mHEIGHT)\n\u001b[1;32m---> 19\u001b[0m \u001b[43mlit_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_onnx\u001b[49m\u001b[43m(\u001b[49m\u001b[43monnx_model_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexport_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Seo\\anaconda3\\envs\\Dron_PytorchLightning\\lib\\site-packages\\torch\\utils\\_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Seo\\anaconda3\\envs\\Dron_PytorchLightning\\lib\\site-packages\\pytorch_lightning\\core\\module.py:1896\u001b[0m, in \u001b[0;36mLightningModule.to_onnx\u001b[1;34m(self, file_path, input_sample, **kwargs)\u001b[0m\n\u001b[0;32m   1893\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1894\u001b[0m         kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexample_outputs\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m(input_sample)\n\u001b[1;32m-> 1896\u001b[0m torch\u001b[38;5;241m.\u001b[39monnx\u001b[38;5;241m.\u001b[39mexport(\u001b[38;5;28mself\u001b[39m, input_sample, file_path, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1897\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain(mode)\n",
      "File \u001b[1;32mc:\\Users\\Seo\\anaconda3\\envs\\Dron_PytorchLightning\\lib\\site-packages\\torch\\onnx\\utils.py:516\u001b[0m, in \u001b[0;36mexport\u001b[1;34m(model, args, f, export_params, verbose, training, input_names, output_names, operator_export_type, opset_version, do_constant_folding, dynamic_axes, keep_initializers_as_inputs, custom_opsets, export_modules_as_functions, autograd_inlining)\u001b[0m\n\u001b[0;32m    189\u001b[0m \u001b[38;5;129m@_beartype\u001b[39m\u001b[38;5;241m.\u001b[39mbeartype\n\u001b[0;32m    190\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mexport\u001b[39m(\n\u001b[0;32m    191\u001b[0m     model: Union[torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mModule, torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mScriptModule, torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mScriptFunction],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    208\u001b[0m     autograd_inlining: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    209\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    210\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Exports a model into ONNX format.\u001b[39;00m\n\u001b[0;32m    211\u001b[0m \n\u001b[0;32m    212\u001b[0m \u001b[38;5;124;03m    If ``model`` is not a :class:`torch.jit.ScriptModule` nor a\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    513\u001b[0m \u001b[38;5;124;03m            All errors are subclasses of :class:`errors.OnnxExporterError`.\u001b[39;00m\n\u001b[0;32m    514\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 516\u001b[0m     \u001b[43m_export\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    517\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    518\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    519\u001b[0m \u001b[43m        \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    520\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexport_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    521\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    522\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    523\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    524\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    525\u001b[0m \u001b[43m        \u001b[49m\u001b[43moperator_export_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moperator_export_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    526\u001b[0m \u001b[43m        \u001b[49m\u001b[43mopset_version\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mopset_version\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    527\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdo_constant_folding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdo_constant_folding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    528\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdynamic_axes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdynamic_axes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    529\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkeep_initializers_as_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_initializers_as_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    530\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcustom_opsets\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_opsets\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    531\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexport_modules_as_functions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexport_modules_as_functions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    532\u001b[0m \u001b[43m        \u001b[49m\u001b[43mautograd_inlining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mautograd_inlining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    533\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Seo\\anaconda3\\envs\\Dron_PytorchLightning\\lib\\site-packages\\torch\\onnx\\utils.py:1612\u001b[0m, in \u001b[0;36m_export\u001b[1;34m(model, args, f, export_params, verbose, training, input_names, output_names, operator_export_type, export_type, opset_version, do_constant_folding, dynamic_axes, keep_initializers_as_inputs, fixed_batch_size, custom_opsets, add_node_names, onnx_shape_inference, export_modules_as_functions, autograd_inlining)\u001b[0m\n\u001b[0;32m   1609\u001b[0m     dynamic_axes \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m   1610\u001b[0m _validate_dynamic_axes(dynamic_axes, model, input_names, output_names)\n\u001b[1;32m-> 1612\u001b[0m graph, params_dict, torch_out \u001b[38;5;241m=\u001b[39m \u001b[43m_model_to_graph\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1613\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1614\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1615\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1616\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1617\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1618\u001b[0m \u001b[43m    \u001b[49m\u001b[43moperator_export_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1619\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_do_constant_folding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1620\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfixed_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfixed_batch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1621\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1622\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdynamic_axes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdynamic_axes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1623\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1625\u001b[0m \u001b[38;5;66;03m# TODO: Don't allocate a in-memory string for the protobuf\u001b[39;00m\n\u001b[0;32m   1626\u001b[0m defer_weight_export \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   1627\u001b[0m     export_type \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _exporter_states\u001b[38;5;241m.\u001b[39mExportTypes\u001b[38;5;241m.\u001b[39mPROTOBUF_FILE\n\u001b[0;32m   1628\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\Seo\\anaconda3\\envs\\Dron_PytorchLightning\\lib\\site-packages\\torch\\onnx\\utils.py:1134\u001b[0m, in \u001b[0;36m_model_to_graph\u001b[1;34m(model, args, verbose, input_names, output_names, operator_export_type, do_constant_folding, _disable_torch_constant_prop, fixed_batch_size, training, dynamic_axes)\u001b[0m\n\u001b[0;32m   1131\u001b[0m     args \u001b[38;5;241m=\u001b[39m (args,)\n\u001b[0;32m   1133\u001b[0m model \u001b[38;5;241m=\u001b[39m _pre_trace_quant_model(model, args)\n\u001b[1;32m-> 1134\u001b[0m graph, params, torch_out, module \u001b[38;5;241m=\u001b[39m \u001b[43m_create_jit_graph\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1135\u001b[0m params_dict \u001b[38;5;241m=\u001b[39m _get_named_param_dict(graph, params)\n\u001b[0;32m   1137\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Seo\\anaconda3\\envs\\Dron_PytorchLightning\\lib\\site-packages\\torch\\onnx\\utils.py:1010\u001b[0m, in \u001b[0;36m_create_jit_graph\u001b[1;34m(model, args)\u001b[0m\n\u001b[0;32m   1005\u001b[0m     graph \u001b[38;5;241m=\u001b[39m _C\u001b[38;5;241m.\u001b[39m_propagate_and_assign_input_shapes(\n\u001b[0;32m   1006\u001b[0m         graph, flattened_args, param_count_list, \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m   1007\u001b[0m     )\n\u001b[0;32m   1008\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m graph, params, torch_out, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1010\u001b[0m graph, torch_out \u001b[38;5;241m=\u001b[39m \u001b[43m_trace_and_get_graph_from_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1011\u001b[0m _C\u001b[38;5;241m.\u001b[39m_jit_pass_onnx_lint(graph)\n\u001b[0;32m   1012\u001b[0m state_dict \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39m_unique_state_dict(model)\n",
      "File \u001b[1;32mc:\\Users\\Seo\\anaconda3\\envs\\Dron_PytorchLightning\\lib\\site-packages\\torch\\onnx\\utils.py:914\u001b[0m, in \u001b[0;36m_trace_and_get_graph_from_model\u001b[1;34m(model, args)\u001b[0m\n\u001b[0;32m    912\u001b[0m prev_autocast_cache_enabled \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mis_autocast_cache_enabled()\n\u001b[0;32m    913\u001b[0m torch\u001b[38;5;241m.\u001b[39mset_autocast_cache_enabled(\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m--> 914\u001b[0m trace_graph, torch_out, inputs_states \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjit\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_trace_graph\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    915\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    916\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    917\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstrict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    918\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_force_outplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    919\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_return_inputs_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    920\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    921\u001b[0m torch\u001b[38;5;241m.\u001b[39mset_autocast_cache_enabled(prev_autocast_cache_enabled)\n\u001b[0;32m    923\u001b[0m warn_on_static_input_change(inputs_states)\n",
      "File \u001b[1;32mc:\\Users\\Seo\\anaconda3\\envs\\Dron_PytorchLightning\\lib\\site-packages\\torch\\jit\\_trace.py:1310\u001b[0m, in \u001b[0;36m_get_trace_graph\u001b[1;34m(f, args, kwargs, strict, _force_outplace, return_inputs, _return_inputs_states)\u001b[0m\n\u001b[0;32m   1308\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(args, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m   1309\u001b[0m     args \u001b[38;5;241m=\u001b[39m (args,)\n\u001b[1;32m-> 1310\u001b[0m outs \u001b[38;5;241m=\u001b[39m ONNXTracedModule(\n\u001b[0;32m   1311\u001b[0m     f, strict, _force_outplace, return_inputs, _return_inputs_states\n\u001b[0;32m   1312\u001b[0m )(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1313\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m outs\n",
      "File \u001b[1;32mc:\\Users\\Seo\\anaconda3\\envs\\Dron_PytorchLightning\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Seo\\anaconda3\\envs\\Dron_PytorchLightning\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Seo\\anaconda3\\envs\\Dron_PytorchLightning\\lib\\site-packages\\torch\\jit\\_trace.py:138\u001b[0m, in \u001b[0;36mONNXTracedModule.forward\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    135\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    136\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m(out_vars)\n\u001b[1;32m--> 138\u001b[0m graph, out \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_graph_by_tracing\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    139\u001b[0m \u001b[43m    \u001b[49m\u001b[43mwrapper\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43min_vars\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodule_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    141\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_create_interpreter_name_lookup_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    142\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstrict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    143\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_force_outplace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    144\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    146\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_return_inputs:\n\u001b[0;32m    147\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m graph, outs[\u001b[38;5;241m0\u001b[39m], ret_inputs[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\Seo\\anaconda3\\envs\\Dron_PytorchLightning\\lib\\site-packages\\torch\\jit\\_trace.py:129\u001b[0m, in \u001b[0;36mONNXTracedModule.forward.<locals>.wrapper\u001b[1;34m(*args)\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_return_inputs_states:\n\u001b[0;32m    128\u001b[0m     inputs_states\u001b[38;5;241m.\u001b[39mappend(_unflatten(in_args, in_desc))\n\u001b[1;32m--> 129\u001b[0m outs\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minner\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtrace_inputs\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    130\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_return_inputs_states:\n\u001b[0;32m    131\u001b[0m     inputs_states[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m (inputs_states[\u001b[38;5;241m0\u001b[39m], trace_inputs)\n",
      "File \u001b[1;32mc:\\Users\\Seo\\anaconda3\\envs\\Dron_PytorchLightning\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Seo\\anaconda3\\envs\\Dron_PytorchLightning\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Seo\\anaconda3\\envs\\Dron_PytorchLightning\\lib\\site-packages\\torch\\nn\\modules\\module.py:1522\u001b[0m, in \u001b[0;36mModule._slow_forward\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1520\u001b[0m         recording_scopes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m   1521\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1522\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m   1524\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m recording_scopes:\n",
      "Cell \u001b[1;32mIn[43], line 38\u001b[0m, in \u001b[0;36mLitCustomModel.forward\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m---> 38\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Seo\\anaconda3\\envs\\Dron_PytorchLightning\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Seo\\anaconda3\\envs\\Dron_PytorchLightning\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Seo\\anaconda3\\envs\\Dron_PytorchLightning\\lib\\site-packages\\torch\\nn\\modules\\module.py:1522\u001b[0m, in \u001b[0;36mModule._slow_forward\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1520\u001b[0m         recording_scopes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m   1521\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1522\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m   1524\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m recording_scopes:\n",
      "\u001b[1;31mTypeError\u001b[0m: CustomModel.forward() takes from 2 to 3 positional arguments but 5 were given"
     ]
    }
   ],
   "source": [
    "model = Swinv2Model.from_pretrained(\"microsoft/swinv2-large-patch4-window12to16-192to256-22kto1k-ft\")\n",
    "lit_model = LitCustomModel(model)\n",
    "# 체크포인트 파일 경로\n",
    "\n",
    "ckpt_path = 'C:\\\\Users\\\\Seo\\\\Desktop\\\\2024_06_Dron_Competition\\\\checkpoints\\\\swinv2-large-resize-epoch=06-train_loss=0.0012-val_score=0.9991.ckpt'\n",
    "\n",
    "# 체크포인트 로드\n",
    "checkpoint = torch.load(ckpt_path)\n",
    "\n",
    "# 모델 상태 로드\n",
    "lit_model.load_state_dict(checkpoint['state_dict'])\n",
    "\n",
    "lit_model.eval()\n",
    "\n",
    "onnx_model_path = 'C:\\\\Users\\\\Seo\\\\Desktop\\\\2024_06_Dron_Competition\\\\swinv2-large-resize-epoch=06-train_loss=0.0012-val_score=0.9991.onnx'\n",
    "\n",
    "input_shape = (CFG.BATCH_SIZE, CFG.CHANNELS, CFG.WIDTH, CFG.HEIGHT)\n",
    "\n",
    "lit_model.to_onnx(onnx_model_path, input_shape, export_params=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Seo\\anaconda3\\envs\\Dron_PytorchLightning\\lib\\site-packages\\torch\\nn\\modules\\lazy.py:181: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
      "  warnings.warn('Lazy modules are a new feature under heavy development '\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'collections.OrderedDict' object has no attribute 'eval'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[38], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m path_to_pytorch_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC:\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mUsers\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mSeo\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mDesktop\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124m2024_06_Dron_Competition\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mswinv2-large-resize-epoch=06-train_loss=0.0012-val_score=0.9991.pth\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m      5\u001b[0m lit_model \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(path_to_pytorch_model)\n\u001b[1;32m----> 6\u001b[0m \u001b[43mlit_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meval\u001b[49m()\n\u001b[0;32m      8\u001b[0m onnx_model_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC:\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mUsers\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mSeo\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mDesktop\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124m2024_06_Dron_Competition\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mswinv2-large-resize-epoch=06-train_loss=0.0012-val_score=0.9991.onnx\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m      9\u001b[0m input_shape \u001b[38;5;241m=\u001b[39m (CFG\u001b[38;5;241m.\u001b[39mBATCH_SIZE, CFG\u001b[38;5;241m.\u001b[39mCHANNELS, CFG\u001b[38;5;241m.\u001b[39mWIDTH, CFG\u001b[38;5;241m.\u001b[39mHEIGHT)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'collections.OrderedDict' object has no attribute 'eval'"
     ]
    }
   ],
   "source": [
    "import onnx\n",
    "model = Swinv2Model.from_pretrained(\"microsoft/swinv2-large-patch4-window12to16-192to256-22kto1k-ft\")\n",
    "lit_model = LitCustomModel(model)\n",
    "path_to_pytorch_model = 'C:\\\\Users\\\\Seo\\\\Desktop\\\\2024_06_Dron_Competition\\\\swinv2-large-resize-epoch=06-train_loss=0.0012-val_score=0.9991.pth'\n",
    "lit_model = torch.load(path_to_pytorch_model)\n",
    "lit_model.eval()\n",
    "\n",
    "onnx_model_path = 'C:\\\\Users\\\\Seo\\\\Desktop\\\\2024_06_Dron_Competition\\\\swinv2-large-resize-epoch=06-train_loss=0.0012-val_score=0.9991.onnx'\n",
    "input_shape = (CFG.BATCH_SIZE, CFG.CHANNELS, CFG.WIDTH, CFG.HEIGHT)\n",
    "\n",
    "example_input = torch.rand(input_shape)\n",
    "\n",
    "torch.onnx.export(lit_model, example_input, onnx_model_path, export_params=True, opset_version=11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv('./open/test.csv')\n",
    "test_df['img_path'] = test_df['img_path'].apply(lambda x: os.path.join('./open', x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not len(test_df) == len(os.listdir('./open/test')):\n",
    "    raise ValueError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize(size=(256,256), interpolation=transforms.InterpolationMode.BICUBIC),\n",
    "    transforms.Normalize(mean=(0.485,0.456,0.406), std=(0.229,0.224,0.225)),\n",
    "])\n",
    "\n",
    "test_collate_fn = CustomCollateFn(test_transform, 'inference')\n",
    "test_dataset = CustomDataset(test_df, 'img_path', mode='inference')\n",
    "test_dataloader = DataLoader(test_dataset, collate_fn=test_collate_fn, batch_size=CFG.BATCH_SIZE*2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fold_preds = []\n",
    "for checkpoint_path in glob('./checkpoints/swinv2-large-resize*.ckpt'):\n",
    "    model = Swinv2Model.from_pretrained(\"microsoft/swinv2-large-patch4-window12to16-192to256-22kto1k-ft\")\n",
    "    lit_model = LitCustomModel.load_from_checkpoint(checkpoint_path, model=model)\n",
    "    trainer = L.Trainer( accelerator='auto', precision=32)\n",
    "    preds = trainer.predict(lit_model, test_dataloader)\n",
    "    preds = torch.cat(preds,dim=0).detach().cpu().numpy().argmax(1)\n",
    "    fold_preds.append(preds)\n",
    "pred_ensemble = list(map(lambda x: np.bincount(x).argmax(),np.stack(fold_preds,axis=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv('./open/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission['label'] = le.inverse_transform(pred_ensemble)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv('./submissions/swinv2_large_resize.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bird",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
